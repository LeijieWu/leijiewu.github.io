---
title: "Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning"
collection: publications
permalink: /publication/2024-01-08-Chiron-A-Robustness-Aware-Incentive-Scheme-for-Edge-Learning-Via-Hierarchical-Reinforcement-Learning
excerpt: 'This paper is about incentive mechanism design in federated learning.'
date: 2024-01-08
venue: 'IEEE Transactions on Mobile Computing (TMC) (CCF-A)'
paperurl: 'https://ieeexplore.ieee.org/document/10382540' 
citation: 'Yi Liu, Song Guo, Yufeng Zhan, <b>Leijie Wu</b>, Zicong Hong, and Qihua Zhou. &quot;Chiron: A Robustness-Aware Incentive Scheme for Edge Learning Via Hierarchical Reinforcement Learning.&quot; <i>IEEE Transactions on Mobile Computing (TMC)</i>. 2024.'
---


Over the past few years, edge learning has achieved significant success in mobile edge networks. Few works have designed incentive mechanism that motivates edge nodes to participate in edge learning. However, most existing works only consider myopic optimization and assume that all edge nodes are honest, which lacks long-term sustainability and the final performance assurance. In this paper, we propose Chiron, an incentive-driven Byzantine-resistant long-term mechanism based on hierarchical reinforcement learning (HRL). First, our optimization goal includes both learning-algorithm performance criteria (i.e., global accuracy) and systematical criteria (i.e., resource consumption), which aim to improve the edge learning performance under a given resource budget. Second, we propose a three-layer HRL architecture to handle long-term optimization, short-term optimization, and byzantine resistance, respectively. Finally, we conduct experiments on various edge learning tasks to demonstrate the superiority of the proposed approach. Specifically, our system can successfully exclude malicious nodes and lazy nodes out of the edge learning participation and achieves 14.96% higher accuracy and 12.66% higher total utility than the state-of-the-art methods under the same budget limit.
